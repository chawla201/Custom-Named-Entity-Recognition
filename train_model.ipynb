{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the custom Named Entity Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfaMMD1-Rzcs",
    "outputId": "74cde306-8261-41b8-de48-eed573337e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "euP7-1Qd4VLx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Iyrzu8sNAS99"
   },
   "outputs": [],
   "source": [
    "text = os.listdir(\"/qubitrics/text\")  #Task 1 data\n",
    "entities = os.listdir(\"/qubitrics/entities\")  #Task 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSzT8vZOTY5U",
    "outputId": "cf2ded29-c877-449e-cd23-1d3e7759e643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n",
      "716\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructing strings for training the model\n",
    "- The reciept text is extracted and reconstructed into strings from `Task 1 - Scanned Reciept Localization` training data\n",
    "- The true values for entities in the exracted text in taken from `Task 2 - Scanned Reciept OCR` training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gMkCyay4AS6t"
   },
   "outputs": [],
   "source": [
    "set_text  = set(text)\n",
    "set_ent = set(entities)\n",
    "\n",
    "training_set = list(set_text.intersection(set_ent)) #to avoid mismatch between text and entity dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUmsVAVq5lCD",
    "outputId": "b21bdd28-1a89-402e-9fbf-96fe699a85d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"filename\", \"text\"])\n",
    "\n",
    "data[\"filename\"] = training_set\n",
    "\n",
    "data_text = []\n",
    "for file in data[\"filename\"]:\n",
    "    rec_text = []\n",
    "    pattern = r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,(.+)\"\n",
    "    with open(f\"/qubitrics/text/{file}\") as f:\n",
    "        f.seek(0)\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rec_text += re.findall(pattern, line)\n",
    "    data_text.append(\" \".join([x.strip() for x in rec_text]))\n",
    "data[\"text\"] = data_text\n",
    "\n",
    "ent_list = []\n",
    "for file in data[\"filename\"]:\n",
    "    with open(f\"/qubitrics/entities/{file}\") as f:\n",
    "        entity_dict = json.load(f)\n",
    "        ent_list.append(entity_dict)\n",
    "data[\"entity_dictionary\"] = ent_list\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "h32Jn2iE5k_a",
    "outputId": "dc92e60a-4991-4235-a3a5-1355022b6137"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>entity_dictionary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X51007339157(1).txt</td>\n",
       "      <td>SANYU STATIONERY SHOP NO. 31G&amp;33G, JALAN SETIA...</td>\n",
       "      <td>{'company': 'SANYU STATIONERY SHOP', 'date': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X51005763940(1).txt</td>\n",
       "      <td>HARVEY NORMAN HARVEY NORMAN M'SIA PARADIGM MAL...</td>\n",
       "      <td>{'company': 'ELITETRAX MARKETING SDN BHD', 'da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X51005757199.txt</td>\n",
       "      <td>POPULAR BOOK CO. (M) SDN BHD (CO. NO. 113825-W...</td>\n",
       "      <td>{'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X51008142063.txt</td>\n",
       "      <td>KEDAI PAPAN YEW CHUAN (0005583085-K) LOT 276 J...</td>\n",
       "      <td>{'company': 'KEDAI PAPAN YEW CHUAN', 'date': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X51005447861.txt</td>\n",
       "      <td>POPULAR BOOK CO. (M) SDN BHD (CO. NO. 113825-W...</td>\n",
       "      <td>{'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>X51005568827.txt</td>\n",
       "      <td>BANH MI CAFE DIMILIKI: BANH MI CAFE SDN BHD 11...</td>\n",
       "      <td>{'company': 'BANH MI CAFE SDN BHD', 'date': '2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>X51005677331.txt</td>\n",
       "      <td>SYARIKAT PERNIAGAAN GIN KEE (81109-A) NO 290, ...</td>\n",
       "      <td>{'company': 'SYARIKAT PERNIAGAAN GIN KEE', 'da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>X51005442338.txt</td>\n",
       "      <td>PASAR MINI JIN SENG 379,JALAN PERMAS SATU, BAN...</td>\n",
       "      <td>{'company': 'PASAR MINI JIN SENG', 'date': '18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>X51006414483.txt</td>\n",
       "      <td>UNIHAKKA INTERNATIONAL SDN BHD 10 APR 2018 18:...</td>\n",
       "      <td>{'company': 'UNIHAKKA INTERNATIONAL SDN BHD', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>X51006619854.txt</td>\n",
       "      <td>99 SPEED MART S/B (519537-X) LOT P.T. 33198, B...</td>\n",
       "      <td>{'company': '99 SPEED MART S/B', 'date': '04-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename  ...                                  entity_dictionary\n",
       "0    X51007339157(1).txt  ...  {'company': 'SANYU STATIONERY SHOP', 'date': '...\n",
       "1    X51005763940(1).txt  ...  {'company': 'ELITETRAX MARKETING SDN BHD', 'da...\n",
       "2       X51005757199.txt  ...  {'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...\n",
       "3       X51008142063.txt  ...  {'company': 'KEDAI PAPAN YEW CHUAN', 'date': '...\n",
       "4       X51005447861.txt  ...  {'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...\n",
       "..                   ...  ...                                                ...\n",
       "711     X51005568827.txt  ...  {'company': 'BANH MI CAFE SDN BHD', 'date': '2...\n",
       "712     X51005677331.txt  ...  {'company': 'SYARIKAT PERNIAGAAN GIN KEE', 'da...\n",
       "713     X51005442338.txt  ...  {'company': 'PASAR MINI JIN SENG', 'date': '18...\n",
       "714     X51006414483.txt  ...  {'company': 'UNIHAKKA INTERNATIONAL SDN BHD', ...\n",
       "715     X51006619854.txt  ...  {'company': '99 SPEED MART S/B', 'date': '04-0...\n",
       "\n",
       "[716 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming data into spaCy compliant format\n",
    "\n",
    "For training a spaCy model for customized named entity recognition, the data must be transformed to the following format<br>\n",
    "`[(input text, entites:[(start_index, end_index, entity_name), (start_index, end_index, entity_name), ...]), ....]` \n",
    "<br>\n",
    "To correcty match the entity values given in the entity dictionary to the phrases in the corresponding text file, I have used:\n",
    "- Phrase Matcher\n",
    "- Regular Expressions\n",
    "Regular expressions are used to identify and match the phrases/tokens that were not matched by the Phrase Matcher to ensure the training data set to be as effective as possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Iq1cxRhG5k8q"
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "id_ent = []\n",
    "\n",
    "nlp_match = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp_match.vocab)\n",
    "for index, row in data.iterrows():\n",
    "    ent_dic = row[\"entity_dictionary\"]\n",
    "    ent = []\n",
    "    phrases = list(ent_dic.values())\n",
    "    patterns = [nlp_match.make_doc(phrase) for phrase in phrases]\n",
    "    matcher.add(\"EntityList\", None, *patterns)\n",
    "\n",
    "    doc = nlp_match(row[\"text\"])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = doc[start:end]\n",
    "            if start>0:\n",
    "                sb = doc[0:start]\n",
    "                start_index=len(sb.text)+1\n",
    "            else:\n",
    "                start_index=0\n",
    "            end_index= start_index+len(span.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for key, value in ent_dic.items():\n",
    "            if value==span.text:\n",
    "                ent_tup=(start_index, end_index, key)\n",
    "                ent.append(ent_tup)\n",
    "    ent_set = {\"company\", \"date\", \"total\", \"address\"}\n",
    "    detected_entities = set([key for start, end, key in ent])\n",
    "    missed_entities = list(ent_set - detected_entities)\n",
    "    if \"total\" in missed_entities:\n",
    "        value = ent_dic[\"total\"]\n",
    "        if len(value)>0:\n",
    "            catch_total = re.search(value, str(row[\"text\"]).replace(\",\", \"\"))\n",
    "            ent_tup = (catch_total.span()[0], catch_total.span()[1], \"total\")\n",
    "            ent.append(ent_tup)\n",
    "    if \"date\" in missed_entities:\n",
    "        value = ent_dic[\"date\"]\n",
    "        if len(value)>0:\n",
    "            catch_date = re.search(value, str(row[\"text\"]))\n",
    "            if catch_date == None:\n",
    "                catch_date = re.search(r\"\\d\\d[-/]*\\d\\d[-/]*\\d\\d\", str(row[\"text\"]))\n",
    "            try:\n",
    "                ent_tup = (catch_total.span()[0], catch_total.span()[1], \"date\")\n",
    "                ent.append(ent_tup)\n",
    "            except:\n",
    "                pass\n",
    "    if \"company\" in missed_entities:\n",
    "        value = ent_dic[\"company\"]\n",
    "        catch_company = re.search(value, str(row[\"text\"]))\n",
    "        if catch_company!=None:\n",
    "            ent_tup = (catch_company.span()[0], catch_company.span()[1], \"company\")\n",
    "            ent.append(ent_tup)\n",
    "        else:\n",
    "            catch_company = re.search(value, str(row[\"text\"]).replace(\".\", \"\"))\n",
    "            if catch_company!=None:\n",
    "                ent_tup = (catch_company.span()[0], catch_company.span()[1], \"company\")\n",
    "                ent.append(ent_tup)\n",
    "    if \"address\" in missed_entities:\n",
    "        try:\n",
    "            value = ent_dic[\"address\"]\n",
    "            catch_address = re.search(value, str(row[\"text\"]))\n",
    "            if catch_address!=None:\n",
    "                ent_tup = (catch_address.span()[0], catch_address.span()[1], \"address\")\n",
    "                ent.append(ent_tup)\n",
    "        except:\n",
    "            pass\n",
    "    id_ent.append(len(ent))\n",
    "    entity_dictionary = {\"entities\": ent}\n",
    "    train_tup = (row[\"text\"], entity_dictionary)\n",
    "    training_data.append(train_tup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqg02Tu963rO",
    "outputId": "0a64b6b6-476f-4435-89e6-253a3df7eb06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the custom NER Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparmeter Values:\n",
    "- **Number of Iterations**: 80\n",
    "- **Dropout**: 0.6\n",
    "- **Minimum Batch Size**: 4\n",
    "- **Maximum Batch Size**: 32\n",
    "- **Compounding factor**: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YVTRlu0DASxu"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = training_data\n",
    "output_dir=\"/content/drive/MyDrive/qubitrics_internship_assignment/model\"\n",
    "n_iter = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tl35KCaxDIni",
    "outputId": "7c8306fd-ddd7-4d33-9f8d-d83338826781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Losses {'ner': 12974.743664056032}\n",
      "1 Losses {'ner': 17605.0854382627}\n",
      "2 Losses {'ner': 22397.8668254816}\n",
      "3 Losses {'ner': 22230.640010044022}\n",
      "4 Losses {'ner': 23507.19024447282}\n",
      "5 Losses {'ner': 23773.06150963734}\n",
      "6 Losses {'ner': 22811.54948925385}\n",
      "7 Losses {'ner': 19172.24888464052}\n",
      "8 Losses {'ner': 17384.020578325573}\n",
      "9 Losses {'ner': 18561.810039345055}\n",
      "10 Losses {'ner': 17603.17206580461}\n",
      "11 Losses {'ner': 17206.517447793653}\n",
      "12 Losses {'ner': 14790.336147851283}\n",
      "13 Losses {'ner': 16355.159918238129}\n",
      "14 Losses {'ner': 13728.183245581147}\n",
      "15 Losses {'ner': 13496.02545978631}\n",
      "16 Losses {'ner': 13030.180207959613}\n",
      "17 Losses {'ner': 11069.342757673901}\n",
      "18 Losses {'ner': 9289.0420544195}\n",
      "19 Losses {'ner': 8272.538787234234}\n",
      "20 Losses {'ner': 8160.740559127125}\n",
      "21 Losses {'ner': 8738.867668556524}\n",
      "22 Losses {'ner': 5865.906072477139}\n",
      "23 Losses {'ner': 5362.072545458739}\n",
      "24 Losses {'ner': 5091.726134207575}\n",
      "25 Losses {'ner': 4252.640293221}\n",
      "26 Losses {'ner': 4769.905745454959}\n",
      "27 Losses {'ner': 3983.477331944797}\n",
      "28 Losses {'ner': 4075.0341932910296}\n",
      "29 Losses {'ner': 4035.938071442165}\n",
      "30 Losses {'ner': 4218.489610879166}\n",
      "31 Losses {'ner': 3294.162067412498}\n",
      "32 Losses {'ner': 3160.5214823200054}\n",
      "33 Losses {'ner': 3180.8612288131017}\n",
      "34 Losses {'ner': 2870.54256662822}\n",
      "35 Losses {'ner': 2733.0919920298425}\n",
      "36 Losses {'ner': 2720.685593548159}\n",
      "37 Losses {'ner': 2868.5006923633473}\n",
      "38 Losses {'ner': 2528.235601844719}\n",
      "39 Losses {'ner': 2357.02237563536}\n",
      "40 Losses {'ner': 2327.0757297021055}\n",
      "41 Losses {'ner': 2315.988309135843}\n",
      "42 Losses {'ner': 2424.4997185054563}\n",
      "43 Losses {'ner': 2359.4566958640175}\n",
      "44 Losses {'ner': 2203.0826161514733}\n",
      "45 Losses {'ner': 2157.034283520184}\n",
      "46 Losses {'ner': 2106.129465855781}\n",
      "47 Losses {'ner': 2096.156715077839}\n",
      "48 Losses {'ner': 2061.5363728781003}\n",
      "49 Losses {'ner': 1904.340810825825}\n",
      "50 Losses {'ner': 1877.4951734715685}\n",
      "51 Losses {'ner': 1958.0205531655506}\n",
      "52 Losses {'ner': 2061.976154819653}\n",
      "53 Losses {'ner': 2002.3984536224796}\n",
      "54 Losses {'ner': 1770.8520726309398}\n",
      "55 Losses {'ner': 1859.346637675035}\n",
      "56 Losses {'ner': 1724.1989830238758}\n",
      "57 Losses {'ner': 1679.5074359386513}\n",
      "58 Losses {'ner': 1913.9296813757685}\n",
      "59 Losses {'ner': 1466.3402120254548}\n",
      "60 Losses {'ner': 1828.590739226441}\n",
      "61 Losses {'ner': 1635.4857501475944}\n",
      "62 Losses {'ner': 1616.5601630809133}\n",
      "63 Losses {'ner': 1656.735226199154}\n",
      "64 Losses {'ner': 1657.873358965196}\n",
      "65 Losses {'ner': 1540.832387237839}\n",
      "66 Losses {'ner': 1480.2851637129686}\n",
      "67 Losses {'ner': 1475.860279848367}\n",
      "68 Losses {'ner': 1525.2032776782885}\n",
      "69 Losses {'ner': 1404.4963078304552}\n",
      "70 Losses {'ner': 1454.621749271041}\n",
      "71 Losses {'ner': 1299.9801711207492}\n",
      "72 Losses {'ner': 1388.952378403285}\n",
      "73 Losses {'ner': 1277.3683468826246}\n",
      "74 Losses {'ner': 1316.1873428006952}\n",
      "75 Losses {'ner': 1160.9929936619653}\n",
      "76 Losses {'ner': 1242.8438474946854}\n",
      "77 Losses {'ner': 1258.05471252093}\n",
      "78 Losses {'ner': 1114.886524973954}\n",
      "79 Losses {'ner': 1199.9347483513025}\n",
      "\n",
      "Saved model to /content/drive/MyDrive/qubitrics_internship_assignment/model\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner, last=True)\n",
    "\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "nlp.begin_training()\n",
    "for itn in range(n_iter):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.01))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        try:\n",
    "            nlp.update(\n",
    "                texts,\n",
    "                annotations, \n",
    "                drop=0.6,\n",
    "                losses=losses,\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"{itn} Losses\", losses)\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"\\nSaved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4KdfBcDWM0X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
